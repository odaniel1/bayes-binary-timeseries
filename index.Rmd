---
title: "Methods for binary time series analysis"
output:
  tint::tintHtml::
    toc: true
    toc_depth: 2
always_allow_html: yes
---
<style>
.math {
  font-size: 12pt;
}
</style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE, fig.margin = TRUE)
library(tufte)
library(tint)
library(knitr)
library(kableExtra)
library(drake)
library(here)
library(tidyverse)

model_plots <- readd(model_plots)

# plot theme
ggplot2::theme_set(
  ggplot2::theme_minimal(base_size = 14) +
  ggplot2::theme(
    panel.grid.minor.y = ggplot2::element_blank(),
    panel.grid.major.y = ggplot2::element_blank(),
    panel.grid.minor.x = ggplot2::element_blank(),
    panel.spacing = ggplot2::unit(1, "lines"),
    strip.text.y = ggplot2::element_blank(),
    legend.position = "bottom"
   )
)

```

`r margin_note("Binary data often also arises through reduction of a complex question to a yes/no simplification: for instance, analysing exam pass/fail rates is seemingly a simpler problem to explore than analysing the full distribution of exam marks.\n In such settings it can be recommended to study the more detailed question so that we are modelling the true data generating process.")`

Analysis of binary data naturally occurs in settings where we wish to understand the relative likelihood of one of two outcomes occurring (often referred to as failure, $0$, and success, $1$). For instance whether or not a newborn child is a boy or a girl (in this context, one might be cautioned against the failure/success terminology).

If we observe several binary outcomes from a population the overall number of successes is binomially distributed; given a sample of $n$ observations with $0 \leq s \leq n$ successes, the classical (maximum likelihood) estimate for the success rate is simply $s/n$, the proportion of the observed data that were successes.

If our data is collected over a period of time,then it is appropriate to perform a time series analysis: allowing the success rate at a given point in time to be correlated with rates in the past and future. Failing to account for this correlation will lead to higher variance in our estimates.

For the purposes of this work, our focus is not so much on using time series as a method of forecasting future behaviour, as it is to explain observed data, and best account for uncertainty in the observations. We are interested in this question in the presence of two challenging conditions:

* The number of observations at each time point is low - meaning that estimates of success rates which don't take into account nearby observations are likely to be too broad to provide insight.

* Data need not arrive at routine intervals - meaning that methods that are based on differencing/neighbouring statistics are not obvious to define.

## Caveat Emptor

This work should be considered exploratory. It is neither exhaustive (there are likely many other models, and I may have overlooked industry standard approaches), nor authoritative (I do not claim to be an expert on time series).

Moreover - the models which are presented include ones that can instantly be ruled out for unsuitability. These are however informative to understand the limitations, and to justify the need for greater complexity.

As presented the models assume no further covariates beyond the time dependence; where it is clear that additional variables could be factored in, this is highlighted in the text.

`r margin_note("David Robinson's book  [Introduction to Empirical Bayes](http://varianceexplained.org/r/empirical-bayes-book/), provides an accessible and practical introduction to binomial modelling in a Bayesian setting, with R.")`

The focus is primarily on Bayesian methods, due in part to my own personal biases and interests, though some of the models (eg. Random Walk model) are facilitated by the flexibility afforded by working with [Stan](https://mc-stan.org/), whilst others are inherently Bayesian in their nature (Gaussian Process models).

At present there is minimal focus on choosing a *best* model: in particular there is no presentation of model fit metrics, or evaluation against held-out data. Rather, where pros and cons are discussed these are to be seen as generic to the method, and not the application.

Any amendments, corrections and suggestions are welcomed as [GitHub issues](https://github.com/odaniel1/bayes-binary-timeseries/issues).

# The Data

`r margin_note("Data is made available through the [Stack Exchange API](https://api.stackexchange.com/docs); this can be queried in R usnig the [stackr](https://github.com/dgrtwo/stackr) package - coincidentally written by the same David Robinson referenced above.")`
For an open source data set that exhibits the features defined above, we consider the acceptance rate of answers to [Stack Exchange](https://stackexchange.com/) questions. Since David Robinson has already been name-checked in multiple side notes, it feels reasonable to pay homage and analyse hist [Stack Overflow](https://stackoverflow.com/users/712603/david-robinson) answers.

```{r, echo = FALSE, fig.cap="The number of questions answered exhibits both low frequency, and the presence of days with no answers leads to intermittency."}
ggplot(data = readd(answers_daily)) +
  geom_point(aes(date, answered)) +
  scale_y_continuous(expand = c(0, 1)) +
  scale_x_date(date_breaks = "2 years", date_labels = "%Y") +
  xlab("") + ylab("Answers (Daily)")
```

For the most part we will work with summary data of the number of answers made, and accepted, by day. A sample of which is as below:

```{r, echo = FALSE}
readd(answers_daily) %>% 
  head %>%
  kable()
```


# Non-Bayesian Methods

## (Aggregated) Binomial proportions

#### [Code Implementation](https://github.com/odaniel1/bayes-binary-timeseries/tree/refactor-drake/R/fit_mle.R)
```{r,echo = FALSE, results = 'asis',  fig.margin = TRUE,fig.cap = "Even at a monthly level there is high variance between months and no discernable trend."}
model_plots$plot[[which(model_plots$method == "monthly-MLE")]]
```

Putting aside the option of treating days as independent, likely the simplest model would be to aggregate data to a level where we are comfortable to assume independence between the periods, and fit simple binomial proportion methods.

The level of aggregation needs to balance sufficient volume of data with the relevance to the question at hand; for instance in this application aggregating by year feels too infrequent, so we consider monthly averages.

At this point acceptance rates and confidence intervals are estimated using Maximum Likelihood.

### Pros
* The method is standard, and well explained in various resources. 
* Easy to implement, and extend to re-fit for multiple independent individuals.

### Cons
* The resulting confidence intervals do not instill confidence: eg. too much inter-period variance.
* We cannot estimate acceptance rates for months which do not have any data.

## Loess Smoothing

```{r,echo =FALSE}
plot <- readd(answers_daily) %>%
  uncount(answered, .id = "answer_no") %>%
  mutate(accepted = 1 * (answer_no <= accepted)) %>%
  ggplot(., aes(date, accepted)) +
  geom_smooth(method= "loess", level = 0.8,
    size =0.5, color = "black", alpha= 0.2) +
  coord_cartesian(ylim = c(0,1)) +
  labs(x = "Date", y = "Acceptance Rate")

plot
```

This is the default smoothing algorithm applied when calling `ggplot2::geom_smooth()` on data sets with fewer than 1,000 observations.

According to [Wikipedia](https://en.wikipedia.org/wiki/Local_regression) it is a form of non-parametric regression which

> [Fits]  simple models to localized subsets of the data to build up a function that describes the deterministic part of the variation in the data, point by point.

The pros and cons described below focus on the use of LOESS within `ggplot2`.

### Pros
* Very easy to use.
* Easy to refit on multiple data sets.
* Provides estimates for periods without observations.

### Cons
* Very easy to use - but hard to explain, and likely that analysts may use without being aware of the underlying assumptions.
* When applied directly to binary data can yield confidence intervals that extend beyond [0,1].
* Does not yield a mathematical formula for the outputs - eg. the analysis is the graph.
* The algorithm is not invariant under data formatting: eg. providing individual binary outcomes vs binomial proportions returns different curves.

## Logistic regression with time as covariate

#### [Code Implementation](https://github.com/odaniel1/bayes-binary-timeseries/tree/refactor-drake/R/fit_bayes_bayes_hier_glm.R)
```{r, results="hide", echo = FALSE, fig.cap="Linear in time"}
model_plots$plot[[which(model_plots$method == "logistic-regression-1")]]
```

```{r, results="hide", echo = FALSE, fig.cap = "Quadratic in time"}
model_plots$plot[[which(model_plots$method == "logistic-regression-2")]]
```

A common first approach to time series modelling is to simply fit a regression model (in our context, a logistic regression model due to the binary data) treating the time coordinate as a continuous covariate for modelling.

Polynomial regression extends this idea by including not only the time coordinate as a covariate, but powers of this value too. This allows us to generalise away from assuming a linear trend, but there is the perennial question of what degree of polynomial to use (with too high a degree leads to over fitting).

More broadly this approach can be extended to look at any number of regression variants: eg. piece wise models, and more generally splines.


### Pros
* Easy to refit on multiple data sets.
* Can easily adapt to include further covariates.
* Flexible to generalisation.

### Cons
* Assumes a parametric form for time dependence, which may not be appropriate.
* Outcome is highly sensitive to the choice of polynomial degree.

## ARIMA Models

It would be remiss not to include at least a comment about ARIMA in an exploration of time series.

Fitting ARIMA models in the presence of our *challenge* conditions appears to be non-trivial. The method hinges on the notion of differencing between sequential observations - however we are primarily interested in the scenario in which there may be gaps in the observed data.

`r margin_note("This appears to go by the name of GLARMA")`
Moreover the traditional ARIMA model is defined for real valued (normal) response variables: so an extension to the setting of generalised linear models is required.

The random walk model discussed below can be considered an implementation of a continuous time ARIMA(0,1,0) model in a Bayesian setting. 

# Bayesian Approaches

## Hierarchical logistic regression

#### [Code Implementation](https://github.com/odaniel1/bayes-binary-timeseries/tree/refactor-drake/R/fit_bayes_bayes_hier_glm.R)
```{r, results="hide", echo = FALSE}
model_plots$plot[[which(model_plots$method == "bayes-hierarchical-glm")]]
```

Hierarchical modelling (in frequentist settings known as *mixed effects modelling*) supposes that the parameters for a set of covariates in a model can be considered to have been sampled from some global distribution.

Typically this is used to facilitate *partial pooling* of information between similar but not identical observations: allowing us to separate out our uncertainty in a primary parameter value, from variation due to small fluctuations in the hierarchical variables. 

Time series appear to be a prototypical setting in which to apply such a model - as we suppose that the accuracy rate on a given day may deviate slightly from some overall accuracy.

The limitation is that this does not directly put a correlation structure on the deviations - meaning that the approach is only really suited to stationary time series: for which arguably much simpler approaches would suffice.

### Pros
* Partial pooling partially accounts for time series effects.

### Cons
* Off-the-shelf applications would only be suitable for stationary time series.

## Propagating Priors

#### [Code Implementation](https://github.com/odaniel1/bayes-binary-timeseries/tree/refactor-drake/R/fit_prior_propogation.R)


`r margin_note("Assuming a prior $\\text{Beta}(\\alpha,\\beta)$, and given an observation of $n$ samples with $s$ successes, the posterior is distributed as $\\text{Beta}( \\alpha + s, \\beta + n - s).$")`

Bayesian calculation for a single binomial samples is particularly straight forward when working with the conjugate prior, the Beta distribution.

It is tempting to consider a time series approach where we propagate forward in time using the posterior distribution at the $(n-1)$-th point in time as the prior distribution for the sample at time $n$.

```{r, results="hide", echo = FALSE}
model_plots$plot[[which(model_plots$method == "prior-propogation")]]
```

The immediate limitation of this approach is that over time the confidence interval narrows due to the accumulation of the entire history of data (whilst this is not guaranteed it is inevitable in most applied settings where there is not high volatility in the underlying success rate).

`r margin_note("If the Jeffrey's prior,  $\\alpha = \\beta = 1/2$ is used, the Bayesian credible interval will align with the frequentist confidence interval.")`
An alternative approach using the simple conjugate prior method is a moving averages approach - however in this instance the model has little advantage to a non-Bayesian moving average model (eg. adapting the first model above). 

### Pros
* Simple to implement.
* Possibly can be extended to a setting with covariates/hierarchical structure (eg. adapting [Robinson's](http://varianceexplained.org/r/empirical-bayes-book/) approach.

### Cons
* Time correlation is one directional - information in future observations cannot be used to inform past estimates.
* Requires time points to be evenly spread - or an assumption that shorter/longer gaps between samples do not effect the correlation.

## Random walk logistic regression 
#### [Code Implementation](https://github.com/odaniel1/bayes-binary-timeseries/tree/refactor-drake/R/fit_bayes_random_walk.R)
```{r, results="hide", echo = FALSE}
model_plots$plot[[which(model_plots$method == "bayes-random-walk")]]
```

### Pros
* x

### Cons
* x

## Gaussian Process Logistic Regression
```{r, results="hide", echo = FALSE}
# model_plots$plot[[which(model_plots$method == "bayes-random-walk-daily")]]
```
### Pros
* x

### Cons
* x