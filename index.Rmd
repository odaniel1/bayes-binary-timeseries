---
title: "Bayesian methods for binary timeseries"
output:
  tint::tintHtml::
    toc: true
    toc_depth: 2
always_allow_html: yes
---
<style>
.math {
  font-size: 12pt;
}
</style>
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, cache = FALSE, fig.margin = TRUE)
library(tufte)
library(tint)
library(knitr)
library(kableExtra)
library(drake)
library(here)
library(tidyverse)

model_plots <- readd(model_plots)

# plot theme
ggplot2::theme_set(
  ggplot2::theme_minimal(base_size = 14) +
  ggplot2::theme(
    panel.grid.minor.y = ggplot2::element_blank(),
    panel.grid.major.y = ggplot2::element_blank(),
    panel.grid.minor.x = ggplot2::element_blank(),
    panel.spacing = ggplot2::unit(1, "lines"),
    strip.text.y = ggplot2::element_blank(),
    legend.position = "bottom"
   )
)

```

`r margin_note("Binary data often also arises through a process of simplifying analysis: eg. replacing a question which could be on a continuous scale, with a cut-off: for instance, analysis of exam pass/fail rates is a simpler problem to explore than the full distribution of exam marks")`

Binary data occurs in many natural settings where we wish to understand the relative likelihood of one of two outcomes occuring (often referred to as failure $0$ and success). For instance whether or not a newborn child is a boy or a girl (in this context, one might be cautioned against the failure/success terminology).

The natural statistic of interest for binary questions is the underlying proportion of successes. Given a sample of $n$ observations with $0 \leq s \leq n$ successes, the classical (maximum likelihood) estimate for the success rate is simply $s/n$, the proportion of the observed data that were successes.

`r margin_note("[Robinson's](http://varianceexplained.org/r/empirical-bayes-book/) introduction to Empirical Bayes through baseball statistics is an excellent resource for learning the basics of binary inference. Unfortunately it does not explore timeseries.")`
In many instances it is likely that this binary data makes up a timeseries: for example in baseball, the proportion of hits a player makes out of all balls they are thrown (apparently known as *At Bats*). In this context we would expect this proportion to vary over time - both due to game specific covariates (eg. who pitched the ball), and temporal affects (eg. improving performance with experience).

In such applications any underlying success rate at a given point in time is likely to be correlated with success rates within the recent past and future. Estimates for the success rate that do not take into account this correlation will therefore likely be overly susceptible to natural variance.

Our focus will be on exploring methods for analysing binary time series under two challenging conditions:

* Where the number of observations at each time point is low - meaning that estimates of success rates which don't take into account nearby observtaions are likely to be too broad to provide insight.

* Where data does not arrive at routine intervals - meaning that methods that are based on differencing/neighbouring statistics are not very easy to define. 


# The Data

`r margin_note("Data is made available through the [Stack Exchange API](https://api.stackexchange.com/docs); this can be queried in R usnig the [stackr](https://github.com/dgrtwo/stackr) package - coincidentally written by the same David Robinson referenced above.")`
For an open source data set that exhibits the features defined above, we consider the acceptance rate of answers to [Stack Exchange](https://stackexchange.com/) questions. In particular we will look at the data of a single user [Xi'an](https://stats.stackexchange.com/users/7224/xian) on Cross Validated (stats.stackexchange) who is at the time of writing the top ranked user for the Bayesian tag.


```{r, echo = FALSE, fig.cap="The number of questions answered in March 2020 illustrates that the data shows both low volumes, and intermittent frequency"}
ggplot(data = readd(answers_daily) %>% filter(between(date, as.Date("2020-03-01"), as.Date("2020-03-31")))) +
  geom_point(aes(date, answered)) +
  scale_y_continuous(expand = c(0, 1)) +
  scale_x_date(date_breaks = "2 days", date_labels = "%d") +
  xlab("Date (March 2020)") + ylab("Answers")
```

A sample of the first 6 rows of the data are shown below; for the most part we work with data that is aggregated to a daily level - we will however consider one model that looks at variation within days at the end.

```{r, echo = FALSE}
readd(answers_daily) %>% 
  head %>%
  kable()
```


# Non-Bayesian Methods

## (Aggregated) Binomial proportions

#### [Code Implementation](https://github.com/odaniel1/bayes-binary-timeseries/tree/refactor-drake/R/fit_mle.R)
```{r,echo = FALSE, results = 'asis',  fig.margin = TRUE,fig.cap = "Even at a monthly level there is high variance between months and no discernable trend."}
model_plots$plot[[which(model_plots$method == "monthly-MLE")]]
```

Putting aside the option of treating days as independent, likely the simplest model would be to aggregate data to a level where we are comfortable to assume independence between the periods, and fit simple binomial proporiton methods.

The level of aggregation needs to balance sufficient volume of data with the relevence to the question at hand; for instance in this application aggregating by year feels too infrequent, so we consider monthly averages.

At this point acceptance rates and confidence intervals are estimated using Maximum Likelihood..



### Pros
* The method is standard, and well explained in various resources. 
* Easy to implement, and extend to re-fit for multiple independent individuals.

### Cons
* The resulting confidence intervals do not instill confidence.
* There is little indication that we can discern trends.
* We cannot estimate acceptance rates for months which do not have any data.

## Loess Smoothing


```{r,echo =FALSE}
plot <- readd(answers_daily) %>%
  uncount(answered, .id = "answer_no") %>%
  mutate(accepted = 1 * (answer_no <= accepted)) %>%
  ggplot(., aes(date, accepted)) +
  geom_smooth(method= "loess", level = 0.8,
    size =0.5, color = "black", alpha= 0.2) +
  coord_cartesian(ylim = c(0,1)) +
  labs(x = "Date", y = "Acceptance Rate")

plot
```

This is the default smoothing algorithm applied when calling `ggplot2::geom_smooth()` on data sets with fewer than 1,000 observations.

According to [Wikipedia](https://en.wikipedia.org/wiki/Local_regression) it is a form of non-parametric regression which

> [Fits]  simple models to localized subsets of the data to build up a function that describes the deterministic part of the variation in the data, point by point.

The pros and cons described below focus on the use of LOESS within `ggplot2`.

### Pros
* Very easy to use.
* Easy to refit on multiple data sets.

### Cons
* When applied directly to binary data can yield confidence intervals that extend beyond [0,1].
* Very easy to use - but hard to explain, and likely that analysts may use without being aware of the underlying assumptions.
* Does not yield a mathematical formula for the outputs - eg. the analysis is the graph.
* The algorithm is not stable to presenting the data as individual binary outcomes, or as a binomial proportion.

## Binary ARIMA

### Pros
* x

### Cons
* x

# Bayesian Approaches

## Hierarchical logistic regression

#### [Code Implementation](https://github.com/odaniel1/bayes-binary-timeseries/tree/refactor-drake/R/fit_bayes_bayes_hier_glm.R)
```{r, results="hide", echo = FALSE}
model_plots$plot[[which(model_plots$method == "bayes-hierarchical-glm")]]
```

Hierarchical modelling (in frequentist settings known as *mixed effects modelling*) supposes that the parameters for a set of covariates in a model can be considered to have been sampled from some global distribution.

Typically this is used to facilitate *partial pooling* of information between similar but not identical observations: allowing us to separate out our uncertainty in a primary parameter value, from variation due to small fluctuations in the hierarchical variables. 

Time series appear to be a prototypical setting in which to apply such a model - as we suppose that the accuracy rate on a given day may deviate slightly from some overall accuracy.

The limitation is that this does not directly put a correlation structure on the deviations - meaning that the approach is only really suited to stationary time series: for which arguably much simpler approaches would suffice.

### Pros
* Partial pooling partially accounts for time series effects.

### Cons
* Off-the-shelf applications would only be suitable for stationary time series.

## Propogating Priors

#### [Code Implementation](https://github.com/odaniel1/bayes-binary-timeseries/tree/refactor-drake/R/fit_prior_propogation.R)


`r margin_note("Assuming a prior $\\text{Beta}(\\alpha,\\beta)$, and given an observation of $n$ samples with $s$ successes, the posterior is distributed as $\\text{Beta}( \\alpha + s, \\beta + n - s).$")`

Bayesian calculation for a single binomial samples is particularly straight forward when working with the conjugate prior, the Beta distribution.

It is tempting to consider a timeseries approach where we propogate forward in time using the posterior distribution at the $(n-1)$-th point in time as the prior distribution for the sample at time $n$.

```{r, results="hide", echo = FALSE}
model_plots$plot[[which(model_plots$method == "prior-propogation")]]
```

The immediate limitation of this approach is that over time the confidence interval narrows due to the accumulation of the entire history of data (whilst this is not guaranteed it is inevitable in most applied settings where there is not high volatility in the underlying success rate).

`r margin_note("If the Jeffrey's prior,  $\\alpha = \\beta = 1/2$ is used, the Bayesian credible interval will align with the frequentist confidence interval.")`
An alternative approach using the simple conjugate prior method is a moving averages approach - however in this instance the model has little advantage to a non-Bayesian moving average model (eg. adapting the first model above). 

### Pros
* Simple to implement.
* Possibly can be extended to a setting with covariates/hierarchical structure (eg. adapting [Robinson's](http://varianceexplained.org/r/empirical-bayes-book/) approach.

### Cons
* Time correlation is one directional - information in future observations cannot be used to inform past estimates.
* Requires time points to be evenly spread - or an assumption that shorter/longer gaps between samples do not effect the correlation.

## Random walk logistic regression 
#### [Code Implementation](https://github.com/odaniel1/bayes-binary-timeseries/tree/refactor-drake/R/fit_bayes_random_walk.R)
```{r, results="hide", echo = FALSE}
model_plots$plot[[which(model_plots$method == "bayes-random-walk")]]
```

## Gaussian Process Logistic Regression
```{r, results="hide", echo = FALSE}
# model_plots$plot[[which(model_plots$method == "bayes-random-walk-daily")]]
```
### Pros
* x

### Cons
* x